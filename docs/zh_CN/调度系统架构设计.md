##  调度系统架构设计
在对系统架构说明之前，我们先来认识一下调度系统常用的名词

### 1.名词解释
**DAG：** 全称Directed Acyclic Graph，简称DAG。工作流中的Task任务以有向无环图的方式执行，从一个或多个起始节点开始，直至到结束节点结束。举例如下图：

![DAG](/docs/zh_CN/images/dag_examples_cn.jpg)


**任务：**  工作流中的一个任务节点。

**任务类型：** 系统可以调度的任务的类型。目前支持有Shell、Hive、Spark、SQL、MR、Python、Procedure，同时计划支持插件扩展。

**节点类型：** 系统的节点类型除了所有的**任务类型**之外，还有子流程节点等

**调度方式：** 系统要求支持基于cron表达式的定时调度、基于依赖的调度和手动调度。支持开始、停止、暂停、恢复、从该节点开始执行、从失败节点开始执行，从指定节点开始执行。

**调度级别：** 调度一般分为秒级、分钟级、小时级、天级、周级、月级共6级调度

**工作流：** 由多个任务节点或子流程组成的一个DAG流程。要注意子流程也是工作流

**工作流定义：** 对数据处理流程的DAG的描述，即数据如何被处理的流程图，通过拖拽相关节点形成可视化流程

**工作流实例：** 工作流实例表示工作流定义的运行，工作流定义每被运行一次，就产生一个对应的工作流实例。

**任务实例：** 工作流实例开始运行后产生的具体任务称作任务实例，比如Shell任务运行后会产生一个Shell任务实例，任务实例有"运行"、"失败"、"重试"、"跳过"、"成功"等运行状态

**依赖：** 

`自依赖：是任务依赖自身的上一个调度运行结果，比如任务T每天运行时都依赖自身上一天的运行结果，那么任务T就是自依赖` </n>
`任务依赖：工作流/节点对别的工作流/节点的依赖，比如周运行任务依赖天运行任务，天运行任务依赖小时运行任务`

**任务优先级：** 任务进入队列后执行的优先级，即在队列中的任务首先执行哪些。支持对工作流设置优先级，也支持对任务设置优先级

### 2.系统架构

#### 2.1 系统架构图

![系统架构图](/docs/zh_CN/images/architecture.jpg)

#### 2.2 架构说明

* **MasterServer:** 

    MasterServer服务，是分布式的无中心设计，MasterServer主要负责任务分发并监控自身和其他MasterServer以及WorkerServer的健康状态。
    MasterServer服务启动时向Zookeeper注册临时节点，通过监听zookeeper节点变化来进行容错处理。
    
    该服务内主要包含:
    
    **Distributed Quartz**分布式调度组件，主要负责定时任务的启停操作，当quartz调起任务后，Master内部会有线程池具体负责处理任务的后续操作。
    
    **MasterSchedulerThread**是一个扫描线程，定时扫描mysql中的command表，获取直接发起的或者是系统容错生成的**启动流程命令**、**从该节点开始执行命令**、**从失败节点开始执行命令**、**恢复暂停流程命令**。
    
    **MasterExecThread**主要是负责具体的工作流启动和操作，控制流程各任务节点的流转。使用**线程池**来控制该服务的工作流和任务的并发数量。
    
    **MasterTaskExecThread**主要监控任务实例的各种运行状态，负责任务的取消、暂停、kill等操作
    
    当然还有心跳线程、子工作流线程等，除此之外MasterServer还有自身负载的监控，避免"忙死"的状态

* **WorkerServer:** 

    WorkerServer服务也是分布式的无中心设计，WorkerServer主要负责任务的执行和对应日志的保存。WorkerServer服务启动时向Zookeeper注册临时节点，并维持心跳。
    该服务包含：
    **FetchTaskThread**用来不断从**Task Queue**中领取任务，并根据任务类型不同调用**TaskScheduleThread**的不同执行器。
    
    **LoggerServer**是一个RPC服务，提供该WorkerServer节点的任务日志查看功能。使用**线程池**来控制该服务最大并发的任务数量。

* **Zookeeper:** 

    Zookeeper服务，系统中的Master和Worker节点都通过Zookeeper来进行集群管理和容错。另外系统还基于Zookeeper进行事件监听和分布式锁。
    我们也曾经基于Redis实现过队列，不过我们希望EasyScheduler依赖到的组件尽量地少，所以最后还是去掉了Redis实现
    
* **Task Queue:** 

    提供任务队列的操作，目前队列也是基于Zookeeper来实现。由于队列中存的信息较少，不必担心队列里数据过多的情况，实际上我们压测过百万级数据存队列，对系统稳定性和性能没影响

* **Alert** 

    提供告警相关接口，接口主要包括**告警**两种类型的告警数据的存储、查询和通知功能。其中通知功能又有**邮件通知**和**SNMP(暂未实现)**两种。

* **API:** 

    API接口层，主要负责处理前端UI层的请求。该服务统一提供RESTful api向外部提供请求服务。
    接口包括工作流的创建、定义、查询、修改、发布、下线、手工启动、停止、暂停、恢复、从该节点开始执行等等。
    
* **UI:** 

    系统的前端页面，提供系统的各种可视化操作界面，详见使用手册部分。

### 2.3 架构设计思想

#### 2.3.1 MasterServer和WorkerServer的设计思想

#### 去中心化vs中心化 

#### 1. 中心化思想
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;中心化的设计理念比较简单，分布式集群中的节点按照角色分工，大体上分为两种角色：
> ![PNG](/docs/zh_CN/images/esr_3.png)
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;Master的角色主要负责任务分发并监督Slave的健康状态，可以动态的将任务均衡到Slave上，以致Slave节点不至于“忙死”或”闲死”的状态。
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;Worker的角色主要负责任务的执行工作并维护和Master的心跳，以便Master可以分配任务给Slave。

#### 2. 中心化思想设计存在的问题：
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;一旦Master出现了问题，则群龙无首，整个集群就崩溃。为了解决这个问题，大多数Master/Slave架构模式都采用了主备Master的设计方案，可以是热备或者冷备，也可以是自动切换或手动切换，而且越来越多的新系统都开始具备自动选举切换Master的能力,以提升系统的可用性。
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;另外一个问题是如果Scheduler在Master上，虽然可以支持一个DAG中不同的任务运行在不同的机器上，但是会产生Master的过负载。如果Scheduler在Slave上，则一个DAG中所有的任务都只能在某一台机器上进行作业提交，则并行任务比较多的时候，Slave的压力可能会比较大。

#### 3. 去中心化
> ![PNG](/docs/zh_CN/images/esr_4.png)
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;去中心化设计里，通常没有Master/Slave的概念，所有的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备down机，都只会影响很小范围的功能。
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;去中心化设计的核心设计在于整个分布式系统中不存在一个区别于其他节点的”管理者”，因此不存在单点故障问题。但由于不存在” 管理者”节点所以每个节点都需要跟其他节点通信才得到必须要的机器信息，而分布式系统通信的不可靠行，则大大增加了上述功能的实现难度。
> ##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;实际上，真正去中心化的分布式系统并不多见。反而动态中心化分布式系统正在不断涌出。在这种架构下，集群中的管理者是被动态选择出来的，而不是预置的，并且集群在发生故障的时候，集群的节点会自发的举行"会议"选举新的"管理者"主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd。

&#8194;

> ##### EasyScheduler的去中心化是Master/Worker注册到Zookeeper中，实现Master集群和Worker集群无中心，并使用Zookeeper分布式锁来选举其中的一台Master或Worker为“管理者”来执行任务。

### 分布式锁实践

##### EasyScheduler使用Zookeeper分布式锁来实现同一时刻只有一台Master执行Scheduler，或者只有一台Worker执行任务的提交。
##### 1. 获取分布式锁的核心流程算法如下：
>![PNG](/docs/zh_CN/images/esr_5.png)

#### 2. EasyScheduler中Scheduler线程分布式锁实现流程图：
>![PNG](/docs/zh_CN/images/esr_6.png)


### 线程不足循环等待问题

- ##### 如果一个DAG中没有子流程，则如果Command中的数据条数大于线程池设置的阈值，则直接流程等待或失败。
- ##### 如果一个大的DAG中嵌套了很多子流程，如下图：
>![PNG](/docs/zh_CN/images/esr_7.png)
>##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;则会产生“死等”状态。MainFlowThread等待SubFlowThread1结束，SubFlowThread1等待SubFlowThread2结束，SubFlowThread2等待SubFlowThread3结束，而SubFlowThread3等待线程池有新线程，则整个DAG流程不能结束，从而其中的线程也不能释放。
>##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;这样就形成的子父流程循环等待的状态。此时除非启动新的Master来增加线程来打破这样的”僵局”，否则调度集群将不能再使用。


- ##### 对于启动新Master来打破僵局，似乎有点差强人意，于是我们提出了以下三种方案来降低这种风险：
>- ##### 计算所有Master的线程总和，然后对每一个DAG需要计算其需要的线程数，也就是在DAG流程执行之前做预计算。因为是多Master线程池，所以总线程数不太可能实时获取。 
>- ##### 对单Master线程池进行判断，如果线程池已经满了，则让线程直接失败。
>- ##### 增加一种资源不足的Command类型，如果线程池不足，则将主流程挂起。这样线程池就有了新的线程，可以让资源不足挂起的流程重新唤醒执行。

- ##### 注意：Master Scheduler线程在获取Command的时候是FIFO的方式执行的。
- ##### 于是我们选择了第三种方式来解决线程不足的问题。


### 容错设计

- ##### EasyScheduler容错设计依赖于Zookeeper的Watcher机制，实现原理如图：
>![PNG](/docs/zh_CN/images/esr_8.png)
>##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;Master监控其他Master和Worker的目录，如果监听到remove事件，则会根据具体的业务逻辑进行流程实例容错或者任务实例容错。

- ##### Master容错流程图：
>![PNG](/docs/zh_CN/images/esr_9.png)
>##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;ZooKeeper  Master容错完成之后则重新由EasyScheduler中Scheduler线程调度，遍历 DAG 找到”正在运行”和“提交成功”的任务，对”正在运行”的任务监控其任务实例的状态，对”提交成功”的任务需要判断Task Queue中是否已经存在，如果存在则同样监控任务实例的状态，如果不存在则重新提交任务实例。


- ##### Worker容错流程图：
>![PNG](/docs/zh_CN/images/esr_10.png)
>##### &#8194;&#8194;&#8194;&#8194;&#8194;&#8194;&#8194;Master Scheduler线程一旦发现任务实例为” 需要容错”状态，则接管任务并进行重新提交。


- ##### 注意：由于” 网络抖动”可能会使得节点短时间内失去和zk的心跳，从而发生节点的remove事件。对于这种情况，我们使用最简单的方式，那就是节点一旦和zk发生超时连接，则直接将Master或Worker服务停掉。


### Logback和gRPC实现日志访问
<div id="日志访问"></div>

- #### 由于Web和Worker不一定在同一台机器上，所以查看日志不能像查询本地文件那样。有两种方案：
  - ##### 将日志放到ES搜索引擎上
  - ##### 通过gRPC通信获取远程日志信息

- #### 介于考虑到尽可能的EasyScheduler的轻量级性，所以选择了gRPC实现远程访问日志信息。
>![PNG](/docs/zh_CN/images/esr_11.png)


- ##### 我们使用自定义Logback的FileAppender和Filter功能，实现每个任务实例生成一个日志文件。


- #### FileAppender实现如下：
>![PNG](/docs/zh_CN/images/esr_12.png)
##### 以…/流程定义id/流程实例id/任务实例id.log的形式生成日志。


- ##### 过滤匹配以TaskLogInfo开始的线程名称：
>![PNG](/docs/zh_CN/images/esr_13.png)


### 总结
>#### 本文从调度出发，介绍了易观研发的大数据分布式工作流调度系统。EasyScheduler在易观数据平台起着中流砥柱的作用。本章着重介绍了EasyScheduler的架构原理及实现思路。
    